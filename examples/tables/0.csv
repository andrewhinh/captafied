Repos,Stars,Forks,"Release Year",Icons,Summary,Description
Transformers,76000,17200,2020,./examples/images/0.png,"Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models.","Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX."
Tapas,894,184,2020,./examples/images/1.png,"TaPas: Weakly Supervised Table Parsing via Pre-training","End-to-end neural table-text understanding models."
CLIP,11200,1700,2021,./examples/images/2.png,"CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs.","Contrastive Language-Image Pre-Training"
DeBERTa,1200,154,2020,./examples/images/3.png,"DeBERTa: Decoding-enhanced BERT with Disentangled Attention","Decoding-enhanced BERT with Disentangled Attention"
DALLE-pytorch,5000,567,2021,./examples/images/4.png,"DALLE-pytorch: Discrete VAEs for Image Text Generation","Discrete VAEs for Image Text Generation"
DeiT,3200,465,2021,./examples/images/5.png,"DeiT: Data-efficient Image Transformers","Data-efficient Image Transformers"
Detr,10200,1900,2020,./examples/images/6.png,"DETR: End-to-End Object Detection with Transformers","End-to-End Object Detection with Transformers"
Flax,3800,448,2020,./examples/images/7.png,"Flax: A neural network library for JAX","A neural network library for JAX"